{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f24cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace Haar cascades with modern deep learning\n",
    "!pip install mediapipe tensorflow\n",
    "# Implement: Age/gender detection, facial landmarks, emotion recognition\n",
    "\n",
    "\n",
    "\n",
    "# Add virtual masks, 3D overlays, face filters\n",
    "!pip install opencv-contrib-python\n",
    "# Create Instagram-style filters in real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6864841a",
   "metadata": {},
   "source": [
    "## Real-time Face Detection and Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2cf8f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting webcam face detection...\n",
      "Press 'q' to quit\n",
      "Face detection stopped\n"
     ]
    }
   ],
   "source": [
    "# Real-time Webcam Face Detection\n",
    "# Essential libraries only\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Load the pre-trained face detection classifier\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "print(\"Starting webcam face detection...\")\n",
    "print(\"Press 'q' to quit\")\n",
    "\n",
    "while True:\n",
    "    # Read frame from webcam\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "    \n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30)\n",
    "    )\n",
    "    \n",
    "    # Draw rectangles around detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, 'Face', (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    \n",
    "    # Display face count on frame\n",
    "    cv2.putText(frame, f'Faces: {len(faces)}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    # Show the frame\n",
    "    cv2.imshow('Real-time Face Detection', frame)\n",
    "    \n",
    "    # Break loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Face detection stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ba5c23",
   "metadata": {},
   "source": [
    "## Advanced Face Detection with Innovative Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87801186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Advanced Face Detection with Innovations!\n",
      "Features: Face Tracking | Emotion Detection | Gesture Recognition | Analytics\n",
      "Press 'q' to quit, 's' to save analytics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nðŸŽ¯ MORE INNOVATIVE FEATURES TO ADD:\\n\\n1. AI-POWERED ENHANCEMENTS:\\n   - Replace Haar cascades with MTCNN or RetinaFace for better accuracy\\n   - Add age and gender prediction using deep learning models\\n   - Implement facial landmark detection for precise feature tracking\\n\\n2. AUGMENTED REALITY FEATURES:\\n   - Virtual masks/filters overlay\\n   - 3D face mesh visualization\\n   - Real-time face morphing effects\\n\\n3. BEHAVIORAL ANALYSIS:\\n   - Attention tracking (looking at camera vs away)\\n   - Blink detection and rate analysis\\n   - Micro-expression detection\\n\\n4. SMART ALERTS SYSTEM:\\n   - Multiple people detection alerts\\n   - Unknown face detection\\n   - Suspicious behavior detection\\n\\n5. ADVANCED DATA ANALYTICS:\\n   - Face clustering and recognition\\n   - Demographic analysis\\n   - Heat maps of face positions\\n   - Time-series analysis of detection patterns\\n\\n6. INTEGRATION FEATURES:\\n   - Save detected faces to database\\n   - Real-time streaming to web dashboard\\n   - Mobile app integration\\n   - Cloud storage for analytics\\n\\n7. PERFORMANCE OPTIMIZATIONS:\\n   - GPU acceleration with OpenCV DNN\\n   - Multi-threading for real-time processing\\n   - Adaptive quality based on hardware\\n\\n8. SECURITY FEATURES:\\n   - Face spoofing detection (liveness detection)\\n   - Privacy mode (blur faces)\\n   - Encrypted face data storage\\n\\nTo implement any of these, you would need additional libraries:\\n- pip install dlib mediapipe tensorflow torch ultralytics\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Advanced Face Detection with Innovative Features\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from collections import deque\n",
    "import json\n",
    "\n",
    "class InnovativeFaceDetector:\n",
    "    def __init__(self):\n",
    "        # Load classifiers\n",
    "        self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "        \n",
    "        # Innovation 1: Face tracking with unique IDs\n",
    "        self.face_tracker = {}\n",
    "        self.next_face_id = 0\n",
    "        self.max_disappeared = 30\n",
    "        \n",
    "        # Innovation 2: Emotion detection colors\n",
    "        self.emotion_colors = {\n",
    "            'neutral': (255, 255, 255),\n",
    "            'happy': (0, 255, 0),\n",
    "            'surprised': (0, 255, 255),\n",
    "            'focused': (255, 0, 255)\n",
    "        }\n",
    "        \n",
    "        # Innovation 3: Face analytics\n",
    "        self.face_analytics = {\n",
    "            'total_faces_detected': 0,\n",
    "            'session_start': time.time(),\n",
    "            'face_sizes': deque(maxlen=100),\n",
    "            'detection_confidence': deque(maxlen=50)\n",
    "        }\n",
    "        \n",
    "        # Innovation 4: Gesture detection\n",
    "        self.gesture_states = {}\n",
    "        \n",
    "        # Innovation 5: Performance monitoring\n",
    "        self.fps_counter = deque(maxlen=30)\n",
    "        self.frame_times = deque(maxlen=30)\n",
    "\n",
    "    def calculate_face_distance(self, face1, face2):\n",
    "        \"\"\"Calculate distance between two face centers\"\"\"\n",
    "        center1 = (face1[0] + face1[2]//2, face1[1] + face1[3]//2)\n",
    "        center2 = (face2[0] + face2[2]//2, face2[1] + face2[3]//2)\n",
    "        return math.sqrt((center1[0] - center2[0])**2 + (center1[1] - center2[1])**2)\n",
    "\n",
    "    def track_faces(self, faces):\n",
    "        \"\"\"Innovation 1: Advanced face tracking with persistent IDs\"\"\"\n",
    "        current_faces = {}\n",
    "        \n",
    "        for face in faces:\n",
    "            x, y, w, h = face\n",
    "            center = (x + w//2, y + h//2)\n",
    "            \n",
    "            # Find closest existing face\n",
    "            best_match = None\n",
    "            min_distance = float('inf')\n",
    "            \n",
    "            for face_id, tracked_face in self.face_tracker.items():\n",
    "                if tracked_face['disappeared'] < self.max_disappeared:\n",
    "                    distance = self.calculate_face_distance(face, tracked_face['bbox'])\n",
    "                    if distance < min_distance and distance < 100:\n",
    "                        min_distance = distance\n",
    "                        best_match = face_id\n",
    "            \n",
    "            if best_match is not None:\n",
    "                # Update existing face\n",
    "                current_faces[best_match] = {\n",
    "                    'bbox': face,\n",
    "                    'center': center,\n",
    "                    'disappeared': 0,\n",
    "                    'duration': self.face_tracker[best_match]['duration'] + 1,\n",
    "                    'first_seen': self.face_tracker[best_match]['first_seen']\n",
    "                }\n",
    "            else:\n",
    "                # New face\n",
    "                current_faces[self.next_face_id] = {\n",
    "                    'bbox': face,\n",
    "                    'center': center,\n",
    "                    'disappeared': 0,\n",
    "                    'duration': 1,\n",
    "                    'first_seen': time.time()\n",
    "                }\n",
    "                self.face_analytics['total_faces_detected'] += 1\n",
    "                self.next_face_id += 1\n",
    "        \n",
    "        # Mark missing faces as disappeared\n",
    "        for face_id, tracked_face in self.face_tracker.items():\n",
    "            if face_id not in current_faces:\n",
    "                if tracked_face['disappeared'] < self.max_disappeared:\n",
    "                    current_faces[face_id] = {\n",
    "                        **tracked_face,\n",
    "                        'disappeared': tracked_face['disappeared'] + 1\n",
    "                    }\n",
    "        \n",
    "        self.face_tracker = current_faces\n",
    "        return current_faces\n",
    "\n",
    "    def detect_emotion_simple(self, face_roi, eyes):\n",
    "        \"\"\"Innovation 2: Simple emotion detection based on facial features\"\"\"\n",
    "        if len(eyes) >= 2:\n",
    "            return 'happy', self.emotion_colors['happy']\n",
    "        elif len(eyes) == 1:\n",
    "            return 'focused', self.emotion_colors['focused']\n",
    "        else:\n",
    "            return 'neutral', self.emotion_colors['neutral']\n",
    "\n",
    "    def detect_head_gesture(self, face_id, current_center):\n",
    "        \"\"\"Innovation 4: Basic head gesture detection\"\"\"\n",
    "        if face_id not in self.gesture_states:\n",
    "            self.gesture_states[face_id] = deque(maxlen=10)\n",
    "        \n",
    "        self.gesture_states[face_id].append(current_center)\n",
    "        \n",
    "        if len(self.gesture_states[face_id]) < 5:\n",
    "            return \"Tracking...\"\n",
    "        \n",
    "        # Analyze movement pattern\n",
    "        positions = list(self.gesture_states[face_id])\n",
    "        \n",
    "        # Calculate horizontal movement\n",
    "        x_movement = positions[-1][0] - positions[0][0]\n",
    "        y_movement = positions[-1][1] - positions[0][1]\n",
    "        \n",
    "        if abs(x_movement) > 30 and abs(y_movement) < 15:\n",
    "            return \"Head Shake\" if x_movement > 0 else \"Head Turn\"\n",
    "        elif abs(y_movement) > 25 and abs(x_movement) < 15:\n",
    "            return \"Nod\" if y_movement > 0 else \"Head Up\"\n",
    "        else:\n",
    "            return \"Still\"\n",
    "\n",
    "    def calculate_face_quality_score(self, face, frame_gray):\n",
    "        \"\"\"Innovation 3: Face quality assessment\"\"\"\n",
    "        x, y, w, h = face\n",
    "        face_roi = frame_gray[y:y+h, x:x+w]\n",
    "        \n",
    "        # Calculate sharpness (Laplacian variance)\n",
    "        laplacian_var = cv2.Laplacian(face_roi, cv2.CV_64F).var()\n",
    "        \n",
    "        # Calculate size score (larger faces are usually better)\n",
    "        size_score = min((w * h) / 10000, 1.0)\n",
    "        \n",
    "        # Calculate brightness score\n",
    "        brightness = np.mean(face_roi)\n",
    "        brightness_score = 1.0 - abs(brightness - 128) / 128\n",
    "        \n",
    "        # Combined quality score\n",
    "        quality_score = (laplacian_var/1000 + size_score + brightness_score) / 3\n",
    "        return min(quality_score, 1.0)\n",
    "\n",
    "    def draw_advanced_ui(self, frame):\n",
    "        \"\"\"Innovation 5: Advanced UI with analytics\"\"\"\n",
    "        height, width = frame.shape[:2]\n",
    "        \n",
    "        # Create semi-transparent overlay for UI\n",
    "        overlay = frame.copy()\n",
    "        \n",
    "        # Analytics panel\n",
    "        cv2.rectangle(overlay, (10, 10), (400, 150), (0, 0, 0), -1)\n",
    "        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)\n",
    "        \n",
    "        # Display analytics\n",
    "        session_time = int(time.time() - self.face_analytics['session_start'])\n",
    "        avg_fps = np.mean(self.fps_counter) if self.fps_counter else 0\n",
    "        \n",
    "        analytics_text = [\n",
    "            f\"Session Time: {session_time//60}m {session_time%60}s\",\n",
    "            f\"Total Faces Detected: {self.face_analytics['total_faces_detected']}\",\n",
    "            f\"Active Faces: {len([f for f in self.face_tracker.values() if f['disappeared'] == 0])}\",\n",
    "            f\"Average FPS: {avg_fps:.1f}\",\n",
    "            f\"Frame Processing: {np.mean(self.frame_times)*1000:.1f}ms\" if self.frame_times else \"0ms\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(analytics_text):\n",
    "            cv2.putText(frame, text, (15, 30 + i*20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "\n",
    "    def run_detection(self):\n",
    "        \"\"\"Main detection loop with all innovations\"\"\"\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        \n",
    "        print(\"ðŸš€ Starting Advanced Face Detection with Innovations!\")\n",
    "        print(\"Features: Face Tracking | Emotion Detection | Gesture Recognition | Analytics\")\n",
    "        print(\"Press 'q' to quit, 's' to save analytics\")\n",
    "        \n",
    "        while True:\n",
    "            start_time = time.time()\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Detect faces\n",
    "            faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(30, 30))\n",
    "            \n",
    "            # Track faces with IDs\n",
    "            tracked_faces = self.track_faces(faces)\n",
    "            \n",
    "            # Process each tracked face\n",
    "            for face_id, face_data in tracked_faces.items():\n",
    "                if face_data['disappeared'] > 0:\n",
    "                    continue  # Skip disappeared faces\n",
    "                \n",
    "                x, y, w, h = face_data['bbox']\n",
    "                center = face_data['center']\n",
    "                \n",
    "                # Calculate face quality\n",
    "                quality_score = self.calculate_face_quality_score(face_data['bbox'], gray)\n",
    "                self.face_analytics['detection_confidence'].append(quality_score)\n",
    "                \n",
    "                # Detect eyes for emotion\n",
    "                face_roi_gray = gray[y:y+h, x:x+w]\n",
    "                eyes = self.eye_cascade.detectMultiScale(face_roi_gray, 1.1, 3)\n",
    "                \n",
    "                # Simple emotion detection\n",
    "                emotion, color = self.detect_emotion_simple(face_roi_gray, eyes)\n",
    "                \n",
    "                # Detect gestures\n",
    "                gesture = self.detect_head_gesture(face_id, center)\n",
    "                \n",
    "                # Draw enhanced face rectangle\n",
    "                thickness = max(2, int(quality_score * 5))\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), color, thickness)\n",
    "                \n",
    "                # Draw face info\n",
    "                duration = face_data['duration'] / 30  # Convert frames to seconds\n",
    "                info_text = f\"ID:{face_id} | {emotion} | {gesture}\"\n",
    "                cv2.putText(frame, info_text, (x, y-30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "                \n",
    "                quality_text = f\"Quality: {quality_score:.2f} | Time: {duration:.1f}s\"\n",
    "                cv2.putText(frame, quality_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)\n",
    "                \n",
    "                # Draw center point\n",
    "                cv2.circle(frame, center, 3, color, -1)\n",
    "                \n",
    "                # Draw eyes\n",
    "                for (ex, ey, ew, eh) in eyes:\n",
    "                    cv2.rectangle(frame, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (0, 255, 0), 1)\n",
    "            \n",
    "            # Update analytics\n",
    "            self.face_analytics['face_sizes'].extend([w*h for (x,y,w,h) in faces])\n",
    "            \n",
    "            # Draw advanced UI\n",
    "            self.draw_advanced_ui(frame)\n",
    "            \n",
    "            # Performance monitoring\n",
    "            frame_time = time.time() - start_time\n",
    "            self.frame_times.append(frame_time)\n",
    "            self.fps_counter.append(1.0 / frame_time if frame_time > 0 else 0)\n",
    "            \n",
    "            cv2.imshow('Advanced Face Detection', frame)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('s'):\n",
    "                self.save_analytics()\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    def save_analytics(self):\n",
    "        \"\"\"Save session analytics to file\"\"\"\n",
    "        analytics_summary = {\n",
    "            'session_duration': int(time.time() - self.face_analytics['session_start']),\n",
    "            'total_faces_detected': self.face_analytics['total_faces_detected'],\n",
    "            'average_face_size': float(np.mean(self.face_analytics['face_sizes'])) if self.face_analytics['face_sizes'] else 0,\n",
    "            'average_detection_confidence': float(np.mean(self.face_analytics['detection_confidence'])) if self.face_analytics['detection_confidence'] else 0,\n",
    "            'average_fps': float(np.mean(self.fps_counter)) if self.fps_counter else 0\n",
    "        }\n",
    "        \n",
    "        filename = f\"face_detection_analytics_{int(time.time())}.json\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(analytics_summary, f, indent=2)\n",
    "        \n",
    "        print(f\"ðŸ“Š Analytics saved to {filename}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    detector = InnovativeFaceDetector()\n",
    "    detector.run_detection()\n",
    "\n",
    "# Additional Innovation Ideas to Implement:\n",
    "\n",
    "\"\"\"\n",
    "ðŸŽ¯ MORE INNOVATIVE FEATURES TO ADD:\n",
    "\n",
    "1. AI-POWERED ENHANCEMENTS:\n",
    "   - Replace Haar cascades with MTCNN or RetinaFace for better accuracy\n",
    "   - Add age and gender prediction using deep learning models\n",
    "   - Implement facial landmark detection for precise feature tracking\n",
    "\n",
    "2. AUGMENTED REALITY FEATURES:\n",
    "   - Virtual masks/filters overlay\n",
    "   - 3D face mesh visualization\n",
    "   - Real-time face morphing effects\n",
    "\n",
    "3. BEHAVIORAL ANALYSIS:\n",
    "   - Attention tracking (looking at camera vs away)\n",
    "   - Blink detection and rate analysis\n",
    "   - Micro-expression detection\n",
    "\n",
    "4. SMART ALERTS SYSTEM:\n",
    "   - Multiple people detection alerts\n",
    "   - Unknown face detection\n",
    "   - Suspicious behavior detection\n",
    "\n",
    "5. ADVANCED DATA ANALYTICS:\n",
    "   - Face clustering and recognition\n",
    "   - Demographic analysis\n",
    "   - Heat maps of face positions\n",
    "   - Time-series analysis of detection patterns\n",
    "\n",
    "6. INTEGRATION FEATURES:\n",
    "   - Save detected faces to database\n",
    "   - Real-time streaming to web dashboard\n",
    "   - Mobile app integration\n",
    "   - Cloud storage for analytics\n",
    "\n",
    "7. PERFORMANCE OPTIMIZATIONS:\n",
    "   - GPU acceleration with OpenCV DNN\n",
    "   - Multi-threading for real-time processing\n",
    "   - Adaptive quality based on hardware\n",
    "\n",
    "8. SECURITY FEATURES:\n",
    "   - Face spoofing detection (liveness detection)\n",
    "   - Privacy mode (blur faces)\n",
    "   - Encrypted face data storage\n",
    "\n",
    "To implement any of these, you would need additional libraries:\n",
    "- pip install dlib mediapipe tensorflow torch ultralytics\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
